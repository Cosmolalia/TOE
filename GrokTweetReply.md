@Grok awesomeâ€”here are the frozen descriptors (300 K) + Eâ‚‰ for the train set. Î±\_T is the local slope dEâ‚‰/dT at 300 K (from Varshni). Polytype noted. No per-row tuning.

```
material,Eg_eV_300K,epsilon_r,mstar_over_me,a0_A,ThetaD_K,alphaT_eV_per_K_300K,polytype
Si,1.12,11.7,0.26,5.431,640,-2.55e-4,diamond
InP,1.344,12.5,0.08,5.8687,425,-3.57e-4,zincblende
GaAs,1.424,12.9,0.063,5.6533,360,-4.52e-4,zincblende
InAs,0.354,15.15,0.023,6.0583,280,NA,zincblende
HgTe,~0.00,20.8,NA,6.460,NA,NA,zincblende   # inverted / zero-gap; NA where not well-defined
InSb,0.17,16.8,0.014,6.479,160,NA,zincblende
```

Notes:

* Si m\* is the conductivity mass from {m\_l=0.98, m\_t=0.19}.
* Î±\_T values shown (Si, InP, GaAs) are d/dT of the Varshni form at 300 K.
* Missing (NA) allowed per prereg: model handles via the Î´(M) spec and a missing-descriptor flag.
* Proceed with your plan: fit linear Î´(M) and Î² on these 6, then predict the blind set.

---

Sources (300 K reference data):
Si (Îµ\_r, m\_l/m\_t, Î˜\_D, aâ‚€; Varshni): Ioffe NSM Si basic + band-structure pages. ([Ioffe Institute][1])
GaAs (Îµ\_r, m\*, Î˜\_D, aâ‚€; Varshni): Ioffe NSM GaAs basic + band-structure. ([Ioffe Institute][2])
InP (Îµ\_r, m\*, Î˜\_D, aâ‚€; Varshni): Ioffe NSM InP basic + band-structure. ([Ioffe Institute][3])
InAs (Îµ\_r, m\*, Î˜\_D, aâ‚€): Ioffe NSM InAs basic. ([Ioffe Institute][4])
InSb (Îµ\_r, m\*, Î˜\_D, aâ‚€): Ioffe NSM InSb basic.
HgTe (aâ‚€â‰ˆ6.46 Ã…, Îµ\_râ‰ˆ20.8, zero/negative gap at RT): Wikipedia HgTe. ([Ioffe Institute][5])

Awesome! here are the **frozen 300 K descriptors + $E_g$** for the train set (no per-row tuning). $\alpha_T$ is the local slope $\mathrm{d}E_g/\mathrm{d}T$ at 300 K from the Varshni form.

material,Eg_eV_300K,epsilon_r,mstar_over_me,a0_A,ThetaD_K,alphaT_eV_per_K_300K,polytype
Si,1.12,11.7,0.26,5.431,640,-2.546e-4,diamond
InP,1.344,12.5,0.08,5.8687,425,-3.567e-4,zincblende
GaAs,1.424,12.9,0.063,5.6533,360,-4.581e-4,zincblende
InAs,0.354,15.15,0.023,6.0583,280,-2.630e-4,zincblende
HgTe,0.00,20.8,NA,6.460,NA,NA,zincblende   # zero/inverted gap at RT
InSb,0.17,16.8,0.014,6.479,160,-3.656e-4,zincblende
```

Notes: $\alpha_T$ is from $E_g(T)=E_g(0)-\alpha\,T^2/(T+\beta)$ via
$\displaystyle \left.\frac{\mathrm{d}E_g}{\mathrm{d}T}\right|_{300\text{K}}=-\alpha\,\frac{T(T+2\beta)}{(T+\beta)^2}$.
Sources: Ioffe NSM (InAs, InSb, InP, Si pages) + standard Varshni params (Si, GaAs, InP).
Your plan is good: fit linear $\delta(M)$ and (optional) $p^{-\beta}$ on these 6, then predict the blind set. ğŸ™

tweet-length

Fair point: v1 maps an observed 
Eg
E
g
	â€‹

 to a prime (descriptive). For blind prediction weâ€™ve frozen a descriptor-only rule: 
xpred=g(M;Î¸)
x
pred
	â€‹

=g(M;Î¸) from 
{Îµr,mâˆ—/me,a0,Î˜D,Î±T,polytype}
{Îµ
r
	â€‹

,m
âˆ—
/m
e
	â€‹

,a
0
	â€‹

,Î˜
D
	â€‹

,Î±
T
	â€‹

,polytype}; then 
p^=PrimeNearest(maxâ¡{2,xpred})
p
^
	â€‹

=PrimeNearest(max{2,x
pred
	â€‹

}) and 
E^g=Ï†+137/p^+Î´(M)/p^Î²
E
^
g
	â€‹

=Ï†+137/
p
^
	â€‹

+Î´(M)/
p
^
	â€‹

Î²
. Weâ€™ll fit 
Î¸,Î´,Î²
Î¸,Î´,Î² on the train-6 you have and you can blind-test the rest.

mini-thread (2 posts)

Youâ€™re right: the nearest-prime map using 
x=137/(Egâˆ’Ï†)
x=137/(E
g
	â€‹

âˆ’Ï†) is descriptive (uses the observed 
Eg
E
g
	â€‹

). For prediction we preregister:

xpred(M)=g(M;Î¸)=c0+c1/Îµr+c2(mâˆ—/me)+c3lnâ¡a0+c4(Î˜D/300)+c5Î±T+c6Î³polytype,
x
pred
	â€‹

(M)=g(M;Î¸)=c
0
	â€‹

+c
1
	â€‹

/Îµ
r
	â€‹

+c
2
	â€‹

(m
âˆ—
/m
e
	â€‹

)+c
3
	â€‹

lna
0
	â€‹

+c
4
	â€‹

(Î˜
D
	â€‹

/300)+c
5
	â€‹

Î±
T
	â€‹

+c
6
	â€‹

Î³
polytype
	â€‹

,

then 
p^=PrimeNearest(maxâ¡{2,xpred})
p
^
	â€‹

=PrimeNearest(max{2,x
pred
	â€‹

}),

E^g=Ï†+137/p^+Î´(M)/p^Î²
E
^
g
	â€‹

=Ï†+137/
p
^
	â€‹

+Î´(M)/
p
^
	â€‹

Î²
.
No per-row tuning; 
Î¸,Î´,Î²
Î¸,Î´,Î² are frozen before the blind set.
2) You already have the train-6 descriptors + 
Eg
E
g
	â€‹

. Fit 
Î¸,Î´,Î²
Î¸,Î´,Î² there, then use descriptors-only on the blind set to produce 
p^
p
^
	â€‹

 and 
E^g
E
^
g
	â€‹

. Weâ€™ll report MAE/RMSE vs controls (nearest-integer, shuffled labels, and 
a+b/p
a+b/p).

quick guidance for you

v1 = descriptive. Itâ€™s fine (and honest) to say: â€œv1 compresses observed gaps into primes; it doesnâ€™t predict unknowns.â€

v2 = predictive. Our fix is the descriptor-only 
p
p predictor 
g(M;Î¸)
g(M;Î¸). Train it once on a small set (with 
Eg
E
g
	â€‹

 known), then use it to pick 
p
p without seeing 
Eg
E
g
	â€‹

 for the blind set. That meets Grokâ€™s standard for a real prediction.

We also included 
Î´(M)
Î´(M) (descriptor-only) and optional 
pâˆ’Î²
p
âˆ’Î²
 scaling to handle low-gap/inverted cases.
